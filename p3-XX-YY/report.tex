\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc} % sempre salve seus arquivos como UTF8
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{color}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\usepackage{subfig}
\usepackage{diagbox}

% load times font
\usepackage{mathptmx}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}

% comandos
\newcommand{\mdc}[1]{\mathrm{mdc}(#1)}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Foot without marker
\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\title{MO446 -- Introduction to Computer Vision  \\ Project 3}
\author{Breno Leite  \\ Guilherme Leite}
\date{05/10/2017}

\begin{document}

\maketitle
\blfootnote{\textit{\textbf{Important note:} The borders seen in the figures are not part of the image, they are figurative information about the starting and ending points of the image. Moreover, all the image scales in this report were changed in order to make the text more readable.}} \\

%% ---------------- Starts here --------------------------------

\textbf{\LARGE Question 2 - Input Data} \\

	The data used in this project was obtained with a cellphone camera recording the magic cube from Figure \ref{fig:magicCube}. Two videos were used, (\textbf{p3-1-0}) shows a translation movement  (affine transformation) of a magic cube in a "blank" background and was used in the experiments of Question 4 - Feature Tracking, (\textbf{p3-1-1}) shows a rotation movement of the same magic cube and was used in Question 5 - Structure from Motion. Figure\\

\begin{figure}[!h]
	\centering
	{
		\setlength{\fboxsep}{1pt}
		\setlength{\fboxrule}{1pt}
		\fbox{\includegraphics[scale=0.1]{report/magicCube}}
	}
	\caption{Magic cube used in the video recordings.}
	\label{fig:magicCube}
\end{figure}

\textbf{\LARGE Question 3 - Keypoint Selection} \\

	The keypoint detectors explored in this project were Harris and SIFT detector, the most noticeable difference between them is the amount of keypoints extracted, the SIFT algorithm extracts way more keypoints than Harris, Figure \ref{fig:keypointSelection} shows the extraction of both detector. To extract the keypoints in Figure \ref{fig:keypointSelectionHarris} using Harris the algorithm took 0.0067 seconds, on the other hand of the spectrum, to extract the keypoints in Figure \ref{fig:keypointSelectionSift} using SIFT the algorithm took 0.0736 seconds presenting a tradeoff between the amount of keypoints and the time to calculate them.\\

\begin{figure}[!h]
	\centering
	\subfloat[Using Harris detector (\textbf{p3-3-1}).]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.30]{output/p3-3-0}}
		}
		\label{fig:keypointSelectionHarris}
	}
	\quad
	\subfloat[Using SIFT detector (\textbf{p3-3-0}).]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.30]{output/p3-3-1}}
		}
		\label{fig:keypointSelectionSift}
	}
	\caption{Difference between keypoint detection methods.}
	\label{fig:keypointSelection}
\end{figure}

	Considering that the keypoints will be used later on to track the object flow it is vital to obtain points regarding only the magic cube in this step. Using Harris detector many corners of the magic cube were selected and as seen in Figure \ref{fig:keypointSelectionHarris} not a single keypoint outside the magic cube boundaries was selected, the same is not true for the SIFT detector, Figure \ref{fig:keypointSelectionSift}, since it's based on gradients a few points outside the magic cube boundaries were selected.
	
\begin{figure}[!h]
	\centering
	{
		\setlength{\fboxsep}{1pt}
		\setlength{\fboxrule}{1pt}
		\fbox{\includegraphics[scale=0.4]{report/out_points}}
	}
	\caption{SIFT extra keypoints.}
	\label{fig:keypointSelectionSiftPoints}
\end{figure}

	Figure \ref{fig:keypointSelectionSiftPoints} highlights in yellow rectangles the "outliers" keypoints detected by SIFT, some cause by shadows in the image and others by sharp gradients beyond the magic cube.\\

%%---------------------------------------------------------------------

\newpage

\textbf{\LARGE Question 4 - Feature Tracking} \\

In this section the results obtained in the optical flow will be shown. The video \textbf{p3-1-0} was used, which contains basically a translation movement. Two different videos were created in this section \textbf{p3-4-0} and \textbf{p3-4-1}, the first will be showing the results using the Harris as keypoint detector, and the second uses SIFT. Both videos are showing the results of our implementation (Left image, green color) and the OpenCV implementation (Righ image, pink color). \\

In \textbf{Question 3} it was shown that the SIFT usually finds more keypoints than the Harris corner detection, however, it is more expensive. A time comparison is shown in Table \ref{table:flowTime}\\


\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& \multicolumn{2}{c|}{Time (seconds)} \\ \hline
		\backslashbox{\textbf{Implementation}}{\textbf{KP Detector}}      & Harris            & SIFT            \\ \hline
		Ours   & 3.79              & 26.20           \\ \hline
		OpenCV & 0.57              & 0.86            \\ \hline
	\end{tabular}
	\caption{Comparison between our implementation and OpenCV's for optical flow.}
	\label{table:flowTime}
\end{table}

As seen the SIFT is really more expensive, the reason for that is not only SIFT is, by its own, less time efficient than Harris but the large number of keypoints increases the cost of finding the optical flow for every keypoint. Because of the better performance, and also for easiness on the visualization of the results the Harris implementation was used in some of the experiments in this section. \\

Picking a good neighbourhood size is essential for the optical flow algorithm, the Figure \ref{fig:keypointSelection} shows some comparisons between different sizes of neighbourhoods. The images were taken from the first, middle, and last frame from the video (left to right). \\

\begin{figure}[!h]
	\centering
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/7s}}
		}
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/7m}}
		}	
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/7l}}
		}	
	}
	
	%---------------
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/15s}}
		}
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/15m}}
		}	
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/15l}}
		}	
	}
		
	%---------------
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/30s}}
		}
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/30m}}
		}	
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/30l}}
		}	
	}
	
	\caption{Three different frames from the video using $7x7$, $15x15$, and $30x30$ neighbourhood, respectively in each row.}
	\label{fig:keypointSelection}
\end{figure}

The image shows how the keypoints normally falls behind from the actual movement of the object, the first row (representing the $7x7$ neighbourhood) is the most affected by this problem. On the other hand, the last row (representing $30x30$) is the most accurate on the points. However, the number of keypoints is really smaller than the others. The reason for this is the border filtering, as the neighbourhood increases more keypoints needs to be removed from the border in order to maintain the algorithm boundaries. This filtering process removes some keypoints, as seen in the last image from the first column (which represents the first frame of the video), note that for the $7x7$ and $15x15$ neighbourhood the image is identical. \\

The $15x15$ neighbourhood has shown the best results, it does not lack the number of keypoints as the $30x30$ neighbourhood and it also does not suffer as much as the $7x7$ neighbourhood from falling behind the object. Because of this the $15x15$ neighbourhood has been selected to generate de video. The Figure \ref{fig:flowVideo} shows the motion flow on the last frame from the video using Harris keypoint detector. \\

\begin{figure}[!h]
	\centering
	\subfloat[Our implementation.]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.3]{report/videoFlow}}
		}
		\label{fig:videoFlowOur}
	}`
	\enskip
	\subfloat[OpenCV implementation.]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.3]{report/videoFlow_OpenCV}}
		}
		\label{fig:videoFlowCV}
	}
	\caption{Last frame from video showing results on optical flows using Harris Keypoint Detector (\textbf{p3-4-0}).}
	\label{fig:flowVideo}
\end{figure}

The Figure \ref{fig:videoFlowOur} shows the optical flow of the object, however, it is noticeable that the keypoints are falling behind the object. This effect could be optimized using other techniques such as pyramids or even recomputing the keypoints after a certain amount of frames. None of this approaches were implemented in this work. As seen the OpenCV implementation tracks the keypoints with more precision than our implementation, see Figure \ref{fig:videoFlowCV}.  \\


%%---------------------------------------------------------------------

\newpage

\textbf{\LARGE Question 5 - Structure from Motion} \\

In this section we have used the OpenCV implementation of motion, the reason is that it is more consistent which enables better reconstruction performance. We have tried our implementation, however, the falling behind keypoints on the video used (\textbf{p3-1-1}) did not gave good results. Moreover, all the SIFT keypoint detector has been used throughout the experiments. As shown before, it detects more keypoints which gives a better reconstruction of the 3D world. \\

We checked the number of points necessary to the reconstruction by a process of trial and error, the number of keypoints given by Harris keypoint detector was not sufficient to make a good 3D reconstruction. We have also satisfied the equation proposed by Morita et. al. and  $2F > P$, in which $F$ is number of frames from the video, and $P$ is the number of good keypoints \blfootnote{http://cs.brown.edu/courses/cs143/2011/proj5/Morita\_Kanade\_PAMI\_1997.pdf}. Good keypoints are those which never goes out of the point of view. \\

The Figure \ref{fig:3dReconstruction}

\end{document}
