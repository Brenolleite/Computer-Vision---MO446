\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc} % sempre salve seus arquivos como UTF8
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{color}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\usepackage{subfig}
\usepackage{diagbox}

% load times font
\usepackage{mathptmx}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}

% comandos
\newcommand{\mdc}[1]{\mathrm{mdc}(#1)}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Foot without marker
\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\title{MO446 -- Introduction to Computer Vision  \\ Project 3}
\author{Breno Leite  \\ Guilherme Leite}
\date{05/10/2017}

\begin{document}

\maketitle
\blfootnote{\textit{\textbf{Important note:} The borders seen in the figures are not part of the image, they are figurative information about the starting and ending points of the image. Moreover, all the image scales in this report were changed in order to make the text more readable.}} \\

%% ---------------- Starts here --------------------------------

\textbf{\LARGE Question 2 - Input Data} \\

	The data used in this project was obtained with a cellphone camera recording ordinary objects. Two videos were used, (\textbf{p3-1-0}) shows a translation movement  (affine transformation) of a magic cube in a "blank" background and was used in the experiments of Question 4 - Feature Tracking, (\textbf{p3-1-1}) shows a rotation movement of the same magic cube and was used in Question 5 - Structure from Motion.\\

\textbf{\LARGE Question 3 - Keypoint Selection} \\

	The keypoint selectors explored in this project were Harris and SIFT selector, the most noticeable difference between them is the amount of keypoints extracted, the SIFT algorithm extracts way more keypoints than Harris, and at first glance could be thought as the best choice, more keypoints means a better flow generation, right? \\

\begin{figure}[!h]
	\centering
	\subfloat[Using Harris. (\textbf{p3-3-1})]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.30]{output/p3-3-0}}
		}
		\label{fig:keypointSelectionHarris}
	}
	\quad
	\subfloat[Using SIFT. (\textbf{p3-3-0})]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.30]{output/p3-3-1}}
		}
		\label{fig:keypointSelectionSift}
	}
	\caption{Difference between keypoint selection methods.}
	\label{fig:keypointSelection}
\end{figure}

	Not quite, as the goal is to extract keypoints only from the object and SIFT extracts extra points, seen circled in Figure \ref{fig:keypointSelectionSiftPoints}, like shadows, the line pulling the cube and small light changes, the selector falls behind when compared with the Harris which is a way faster algorithm as shown in Table \ref{table:timeHarrisSift}, and is able to extract only the keypoints regarding the object accomplishing our goal at this stage, as seen in Figure \ref{fig:keypointSelectionHarris}. \\

\begin{figure}[!h]
	\centering
	{
		\setlength{\fboxsep}{1pt}
		\setlength{\fboxrule}{1pt}
		\fbox{\includegraphics[scale=0.4]{report/out_points}}
	}
	\caption{SIFT extra keypoints.}
	\label{fig:keypointSelectionSiftPoints}
\end{figure}

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& \multicolumn{3}{c|}{\textbf{Time (seconds)}} \\ \hline
		\backslashbox{\textbf{Average of}}{\textbf{Selector}}    & \textbf{Harris}         & \textbf{SIFT}          & \textbf{-1}      \\ \hline
		\textbf{5 runs}  & 0.0067      & 0.0736      & -1     \\ \hline
		\textbf{-1} & -1      & -1       & -1     \\ \hline
	\end{tabular}
	\caption{Comparison between our implementation and OpenCV convolution time.}
	\label{table:timeHarrisSift}
\end{table}

%%---------------------------------------------------------------------

\newpage

\textbf{\LARGE Question 4 - Feature Tracking} \\

In this section the results obtained in the optical flow will be shown. The video \textbf{p3-1-0} was used, which contains basically a translation movement. Two different videos were created in this section \textbf{p3-4-0} and \textbf{p3-4-1}, the first will be showing the results using the Harris as keypoint selector, and the second uses SIFT. Both videos are showing the results of our implementation (Left image, green color) and the OpenCV implementation (Righ image, pink color). \\

In \textbf{Question 3} it was shown that the SIFT usually finds more keypoints than the Harris corner detection, however, it is more expensive. A time comparisons is shown in Table \ref{table:flowTime} \\


\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& \multicolumn{2}{c|}{Time (seconds)} \\ \hline
		\backslashbox{\textbf{Implementation}}{\textbf{KP Selector}}      & Harris            & SIFT            \\ \hline
		Ours   & 3.79              & 26.20           \\ \hline
		OpenCV & 0.57              & 0.86            \\ \hline
	\end{tabular}
	\caption{Comparison between our implementation and OpenCV for optical flow.}
	\label{table:flowTime}
\end{table}

As seen the SIFT is really more expensive, the reason for that is not only SIFT is, by its own, less time efficient than Harris. The large number of keypoints increases the cost of finding the optical flow for every keypoint. Because of the better performance, and also for easiness on the visualization on the results the Harris implementation was used in some of the experiments in this section. \\

The choose of a good neighborhood is essential for the optical flow algorithm, the Figure \ref{fig:keypointSelection} shows some comparisons between different size of neighborhood. The images were taken from the first, middle, and last frame from the video (left to right). \\

\begin{figure}[!h]
	\centering
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/7s}}
		}
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/7m}}
		}	
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/7l}}
		}	
	}
	
	%---------------
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/15s}}
		}
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/15m}}
		}	
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/15l}}
		}	
	}
		
	%---------------
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/30s}}
		}
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/30m}}
		}	
	}
	\enskip
	{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.2]{report/30l}}
		}	
	}
	
	\caption{Three different frames from the video using $7x7$, $15x15$, and $30x30$ neighborhood, respectively in each row.}
	\label{fig:keypointSelection}
\end{figure}

The image shows how the keypoints normally falls behind from the actual movement of the object, the first row (representing the $7x7$ neighborhood) is the most affected by this problem. In other hand, the last row (representing $30x30$) is the most accurate on the points. However, the number of keypoints is really smaller than the others. The reason for this is the border filtering, as the neighborhood increases more keypoints needs to be removed from the border in order to maintain the algorithm boundaries. This filtering process removes some keypoints, as seen in the last image from the first column (which represents the first frame of the video), note that for the $7x7$ and $15x15$ neighborhood the image is identical. \\

The $15x15$ neighborhood has shown the best results, it does not lack the number of keypoints as the $30x30$ neighborhood and it also does not suffer as much as the $7x7$ neighborhood from falling behind the object. Because of this the $15x15$ neighborhood has been selected to generate de video. The Figure \ref{fig:flowVideo} shows the motion flow on the last frame from the video using Harris keypoint selector. \\

\begin{figure}[!h]
	\centering
	\subfloat[Our implementation.]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.3]{report/videoFlow}}
		}
		\label{fig:videoFlowOur}
	}
	\enskip
	\subfloat[OpenCV implementation.]{
		{
			\setlength{\fboxsep}{1pt}
			\setlength{\fboxrule}{1pt}
			\fbox{\includegraphics[scale=0.3]{report/videoFlow_OpenCV}}
		}
		\label{fig:videoFlowCV}
	}
	\caption{Last frame from video showing results on optical flows using Harris Keypoint Detector (\textbf{p3-4-0}).}
	\label{fig:flowVideo}
\end{figure}

The Figure \ref{fig:videoFlowOur} shows the optical flow of the object, however, it is noticeable that the keypoints are falling behind the object. This effect could be optimized using other techniques such pyramids or even recomputing the keypoints after a certain amount of frames. None of this approaches were implemented in this work. As seen the OpenCV implementation tracks the keypoints with more precision than our implementation, see Figure \ref{fig:videoFlowCV}.  \\


%%---------------------------------------------------------------------

\newpage

\textbf{\LARGE Question 5 - Structure from Motion} \\

\end{document}
